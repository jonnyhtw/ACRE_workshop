{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Synoptic types in Python: an analyis of circulation regimes over the New Zealand region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A definitive introduction to the concept of synoptic types (also *weather regimes*, *circulation regimes*) is beyond the scope of this session, again a good reference \n",
    "is [Michelangeli, P.-A., Vautard, R., & Legras, B. (1995). Weather Regimes: Recurrence and Quasi Stationarity. Journal of Atmospheric Sciences, 52, 1237–1256.](http://journals.ametsoc.org/doi/abs/10.1175/1520-0469%281995%29052%3C1237%3AWRRAQS%3E2.0.CO%3B2)\n",
    "\n",
    "In short: \n",
    "\n",
    "+ there is day to day *variability* in the patterns of atmospheric circulation at all scales from regional to global   \n",
    "        \n",
    "        \n",
    "+ this variability is NOT random \n",
    "\n",
    "        \n",
    "+ at the regional scale particularly, some configuration are preferred --> \"regimes\"  \n",
    "\n",
    "        \n",
    "+ in the non-linear dynamics jargon, these preferred, recurrent configurations of atmospheric circulation are called \"attractors basins\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An illustration of the concept of *attractor basins* with the [Lorenz's system](https://en.wikipedia.org/wiki/Lorenz_system)\n",
    "                                                               \n",
    "Lorenz first discovered chaos by accident while developing a simple mathematical model of atmospheric convection, using three ordinary differential equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dot{x} & = \\sigma(y-x) \\\\\n",
    "\\dot{y} & = \\rho x - y - xz \\\\\n",
    "\\dot{z} & = -\\beta z + xy\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "He found that nearly indistinguishable initial conditions could produce completely divergent outcomes, rendering weather prediction impossible beyond a time horizon of about a fortnight.\n",
    "\n",
    "Now known as the **Lorenz System**, this model demonstrates chaos at certain parameter values, below is an animation of its solutions\n",
    "\n",
    "This animation has been taken from the implementation of the Lorenz's system in Python, by Geoff Boeing, available at [https://github.com/gboeing/lorenz-system/blob/master/lorenz-system-attractor-animate.ipynb](https://github.com/gboeing/lorenz-system/blob/master/lorenz-system-attractor-animate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](./images/animated-lorenz-attractor.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there is clearly 2 **preferred regions** of the phase space that are visited by the trajectories. You can think of these as **recurrent regimes**. It is thought that at the regional scale, the atmospheric circulation \n",
    "follows a similar dynamic, with *N* (finite number of) attractor basins: regions of the phase space which are more likely than other to be visited, i.e. preferred **circulation regimes**, circulation 'types' or 'synoptic' types to refer to their spatial extent. \n",
    "\n",
    "The challenge is to determine these regimes from the data (e.g. daily 'maps' of geopotential)  \n",
    "\n",
    "A common approach is to use [clustering](https://en.wikipedia.org/wiki/Cluster_analysis): i.e. extract N 'clusters' which group days which are 'close' to each other according to some distance criteria (e.g. but not limited to Euclidean distance), many methods exist to do that, from hierarchical clustering and non-hierarchical techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance to historical climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ day to day **weather** (daily temperatures, rainfall, pressure) related to weather regimes \n",
    "\n",
    "\n",
    "+ The occupation statistics of these regimes (frequency, persistence, preferred transitions) are modulated by large-scale climate modes (ENSO, SAM, etc)   \n",
    "\n",
    "\n",
    "+ i.e. weather regimes provide the *link* between climate modes and local scale, daily *weather*  \n",
    "\n",
    "\n",
    "+ can help to better understand non-linearities, assymetries in the regional signals of global climate modes  \n",
    "\n",
    "\n",
    "+ can help 'reconstruct' likely circulation scenarios from a network of daily weather observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "In this notebook we will try and replicate some of the results of the study by:  \n",
    "\n",
    "**Kidson, 2000: An analysis of New Zealand synoptic types and their use in defining weather regimes, IJC, Volume 20, Issue 3, 15 March 2000, Pages 299–316**\n",
    "\n",
    "who used [k-means](https://en.wikipedia.org/wiki/K-means_clustering) clustering to derive 12 synoptic types (The \"Kidson Types\") over the New Zealand region using reanalysed (NCEP / NCAR, *aka* NCEP 1) 1000 hPa geopotential from 1958 to 1997. \n",
    "\n",
    "The **Kidson types** have been used in a wide variety of studies (from subseasonal forecasting to paleoclimatology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the stuff we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downloading the daily NCEP / NCAR HGT from the ESRL and extract a spatial domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as the NCEP / NCAR data is available via FTP rather than HTTP, we will use [cURL](https://curl.haxx.se/)  \n",
    "\n",
    "You can download a **cURL** binary for your platform from [https://curl.haxx.se/dlwiz/](https://curl.haxx.se/dlwiz/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/pressure/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographical domain and level to extract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that the latitude variable is varying from North to South"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lonmin = 160\n",
    "latmin = -55\n",
    "lonmax = 185\n",
    "latmax = -25\n",
    "level = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### directory where to save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opath = '../data/NZ/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from subprocess import call\n",
    "# for y in range(1958, 2016 + 1): \n",
    "#     filename = \"hgt.{}.nc\".format(y)\n",
    "#     cmd = \"curl --silent {}/{} -o {}/{}\".format(base_url, filename, opath, filename)\n",
    "#     r = call(cmd, shell=True)\n",
    "#     if r != 0: \n",
    "#         print(\"something went wrong with the download of hgt.{}.nc\".format(y))\n",
    "#         pass\n",
    "#     else: \n",
    "#         dset = xr.open_dataset('{}/{}'.format(opath, filename))\n",
    "#         dset = dset.sel(lat=slice(latmax, latmin), lon=slice(lonmin,lonmax), level=level)\n",
    "#         dset = dset.squeeze()\n",
    "#         os.remove('{}/{}'.format(opath, filename))\n",
    "#         dset.to_netcdf('{}/{}'.format(opath, filename))\n",
    "#         dset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the list of files in a Python list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lfiles = glob(os.path.join(opath, 'hgt*.nc')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lfiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set a random seed to ensure reproducibility of the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reads in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = xr.open_mfdataset(lfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select the period indicated to be utlized in the Kidson 2000 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = dset.sel(time=slice('1958-1-1','1997-6-30'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculates the time mean over the whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsetm = dset.mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapping with [cartopy](http://scitools.org.uk/cartopy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat = dset.lat\n",
    "lon = dset.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lons, lats = np.meshgrid(lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "central_longitude = 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj = ccrs.PlateCarree(central_longitude=central_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8), subplot_kw={'projection':proj})\n",
    "\n",
    "ax.coastlines('10m')\n",
    "\n",
    "c = ax.contour(lons - central_longitude, lats, dsetm['hgt'], np.arange(-100, 200, 5))\n",
    "\n",
    "plt.clabel(c, fmt='%i')\n",
    "\n",
    "ax.set_extent([lon.data.min() - central_longitude, lon.data.max() - central_longitude, lat.data.min(), lat.data.max()], crs=proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_map(X, lons, lats, vmin=-250, vmax=250, step=10, ax=None, central_longitude=180., fmt='%i'): \n",
    "    \n",
    "    from numpy import ma\n",
    "    \n",
    "    if not(ax): \n",
    "        central_longitude = 180.\n",
    "        proj = ccrs.PlateCarree(central_longitude=central_longitude)\n",
    "        f, ax = plt.subplots(figsize=(10,8), subplot_kw={'projection':proj})\n",
    "        \n",
    "    proj = ccrs.PlateCarree(central_longitude=central_longitude)\n",
    "    \n",
    "    ax.coastlines('10m')\n",
    "            \n",
    "    if X.min() < 0 and X.max() > 0: \n",
    "        p = ax.contour(lons - central_longitude, lats, ma.masked_less(X,0), np.arange(0, vmax + step, step), colors='r')\n",
    "        n = ax.contour(lons - central_longitude, lats, ma.masked_greater(X,0), np.arange(vmin, 0, step), colors='b')\n",
    "        \n",
    "        ax.contour(lons - central_longitude, lats, X, np.array([0]), colors='k')\n",
    "        \n",
    "        plt.clabel(p, fmt=fmt)\n",
    "        plt.clabel(n, fmt=fmt) \n",
    "    elif X.min() < 0 and X.max() < 0: \n",
    "        n = ax.contour(lons - central_longitude, lats, X, np.arange(vmin, vmax + step, step), colors='b')\n",
    "        plt.clabel(n, fmt=fmt)\n",
    "    else: \n",
    "        p = ax.contour(lons - central_longitude, lats, X, np.arange(vmin, vmax + step, step), colors='r')\n",
    "        plt.clabel(p, fmt=fmt)        \n",
    "                            \n",
    "    ax.set_extent([lon.data.min() - central_longitude, lon.data.max() - central_longitude, lat.data.min(), lat.data.max()], crs=proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_map(dsetm['hgt'].data, lons, lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt = dset['hgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we now need to go from 3D (time, lat, lon) to 2D (time, space [lat X lon]) to perform the PCA (EOF) analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt_stacked = hgt.stack(latlon=('lat', 'lon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(hgt_stacked.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we now need to LOAD the dataset in memory in order to perform the PCA (EOF analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt_stacked.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(hgt_stacked.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = hgt_stacked.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of variables (features) is 143 (13 points in latitude * 11 points in longitude): while we can attend to perform our cluster analysis directly on this matrix, \n",
    "it is expensive computationally, i.e. each day (observation) is associated to a point with 143 coordinates, we need to find clusters in 143 dimensions ...\n",
    "\n",
    "The approach generally taken is to **reduce the dimensionality** of the original dataset using methods such as [Principal Component (or Empirical Orthoginal Function) Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). This is the approach that has been used by Kidson, and we will try and follow the way he processed the data as described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do PCA / EOF analysis of daily geopotential fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the PCA, we will use the excellent (not enough superlatives here !) [scikit-learn](http://scikit-learn.org/) Machine Learning library  \n",
    "\n",
    "**Scikit-learn** implements a wide array of [supervised](http://en.wikipedia.org/wiki/Supervised_learning) and [unsupervised](http://en.wikipedia.org/wiki/Unsupervised_learning) Machine Learning algorithms, \n",
    "\n",
    "**Unsupervised** refers to algorithms which learn from a training set of unlabeled examples, using the features of the inputs to categorize inputs together according to some statistical criteria.\n",
    "\n",
    "Unsupervised learning algorithms are usually separated into :\n",
    "\n",
    "+ [Dimensionality reduction](http://scikit-learn.org/stable/modules/decomposition.html#decompositions) , whicn *learns* a more compact representation  of the data (i.e. reducing the dimensions).\n",
    "\n",
    "+ [Clustering](http://scikit-learn.org/stable/modules/clustering.html#clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn exposes a very useful [preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) submodule, which allows to do various transformations on your input data, in this case we will *standardisation* the dataset, removing the mean and dividing by the standard deviation ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler  = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pca class itself is in the *decomposition* submodule of scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skpca = pca.PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skpca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1,21), skpca.explained_variance_ratio_[0:20]*100)\n",
    "ax.plot(range(1,21), skpca.explained_variance_ratio_[0:20]*100,'ro')\n",
    "ax.grid(ls=':')\n",
    "ax.set_xticks(range(1,21)); \n",
    "ax.set_xlabel('PC#');\n",
    "ax.set_ylabel(\"% variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In his paper, Kidson keeps the first 5 Principal Components and perform the cluster analysis on the subspace spanned by these 5 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ipc = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skpca.explained_variance_ratio_[:ipc].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together they explain more than 90% of the total variance contained in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCs = skpca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCs = PCs[:,:ipc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the EOFS contain the spatial patterns associated with each PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EOFs = skpca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EOFs = EOFs[:ipc,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EOFs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EOFs_r = EOFs.reshape((ipc, len(lat), len(lon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_map(EOFs_r[0,:,:], lons, lats, vmin=-0.2, vmax=0.2, step=0.010, fmt='%4.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his paper, not clear whether Kidson *scaled* (standardised) the PCs or not, trying with and without standardisation seem to indicated he DID NOT standardise the PCs prior to the cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaler_PCs = StandardScaler()\n",
    "# scaler_PCs.fit(PCs)\n",
    "# PCs_std = scaler_PCs.transform(PCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCdf = pd.DataFrame(PCs, index = dset['time'], \\\n",
    "                    columns = [\"PC%s\" % (x) for x in range(1, PCs.shape[1] +1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCdf.plot(subplots=True, figsize=(12,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-means clustering class is found in the [clustering](http://scikit-learn.org/stable/modules/clustering.html#clustering) submodule of scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify the number of clusters here ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nclusters = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialise the KMeans class with the parameters, n_jobs=-1 means the computations will be distributed accross the cores of your machine (if you have a multicore CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters=nclusters, n_init=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans.fit(PCdf.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `kmeans.labels_` contains the unique labels for each day: i.e. a number from 0 to nclusters-1 indicating to which cluster (regime) each day belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.unique(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we put that into a Pandas DataFrame and assign the corresponding date to each day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(kmeans.labels_, index=dset['time'], columns=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = labels.query('cluster == {}'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbdays = len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = dset.sel(time=index.index).mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "nbdays = []\n",
    "for c in range(nclusters): \n",
    "    index = labels.query('cluster == {}'.format(c)) \n",
    "    nbdays.append(len(index))\n",
    "    cluster = dset.sel(time=index.index).mean('time')\n",
    "    clusters.append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = xr.concat(clusters, dim='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = clusters['hgt'].plot.contour(x='lon', y='lat', col='cluster', col_wrap=3, levels=np.arange(-150,200,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(nbdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(nrows=3, ncols=4, figsize=(10,8), subplot_kw={'projection':proj})\n",
    "f.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "axes = axes.flatten() \n",
    "for c in range(nclusters): \n",
    "    ax = axes[c]\n",
    "    clus = clusters.sel(cluster=c)\n",
    "    make_map(clus['hgt'], lons, lats, step=25, ax=ax)\n",
    "    ax.text(0.05, 0.9, \"{}: {:3.2f}%\".format(c, nbdays[c] / sum(nbdays) * 100), transform=ax.transAxes, bbox=dict(facecolor='w', alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.savefig('./images/Kidson_clusters.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!open ./images/Kidson_Archetypes.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('./images/Kidson_Archetypes.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at the seasonal distribution of the synoptic types / weather regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(nrows=4, ncols=3, figsize=(10,14))\n",
    "axes = axes.flatten() \n",
    "for c in range(nclusters): \n",
    "    ax = axes[c]\n",
    "    cf = labels.query('cluster == {}'.format(c))\n",
    "    # in percentage\n",
    "    ((cf.groupby(cf.index.month).count()) / len(cf) * 100).plot(kind='bar', width=1, ax=ax, legend=None)\n",
    "    # in number of days\n",
    "#     cf.groupby(cf.index.month).count().plot(kind='bar', width=1, ax=ax, legend=None)\n",
    "    ax.set_ylim(0, None)\n",
    "    ax.grid(ls=':')\n",
    "    ax.text(0.05, 0.9, 'cluster {}'.format(c), transform=ax.transAxes, bbox=dict(facecolor='w', alpha=0.5))\n",
    "    ax.set_xticklabels(list('JFMAMJJASOND'), rotation=0)\n",
    "    ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
